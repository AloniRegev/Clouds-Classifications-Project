{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AloniRegev/Defense-Against-Adversarial-Examples-in-NN/blob/main/DL_project_adversary_attacks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XA7jz320MgQ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dmu0PDC0zBqd",
        "outputId": "68bd93e4-ff92-4ce9-b239-e89c8e04689d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available. Training on GPU\n",
            "Files already downloaded and verified\n",
            " trainset: Dataset CIFAR10\n",
            "    Number of datapoints: 50000\n",
            "    Root location: ./data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               RandomCrop(size=(32, 32), padding=4)\n",
            "               RandomHorizontalFlip(p=0.5)\n",
            "               ToTensor()\n",
            "               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.243, 0.261))\n",
            "           )\n",
            "Files already downloaded and verified\n",
            "train set len 40000\n",
            "validation set len 10000\n",
            "test set len 10000\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 64, 32, 32]          18,496\n",
            "              ReLU-5           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-6           [-1, 64, 16, 16]               0\n",
            "            Conv2d-7          [-1, 128, 16, 16]          73,856\n",
            "       BatchNorm2d-8          [-1, 128, 16, 16]             256\n",
            "              ReLU-9          [-1, 128, 16, 16]               0\n",
            "           Conv2d-10          [-1, 128, 16, 16]         147,584\n",
            "             ReLU-11          [-1, 128, 16, 16]               0\n",
            "        MaxPool2d-12            [-1, 128, 8, 8]               0\n",
            "        Dropout2d-13            [-1, 128, 8, 8]               0\n",
            "           Conv2d-14            [-1, 256, 8, 8]         295,168\n",
            "      BatchNorm2d-15            [-1, 256, 8, 8]             512\n",
            "             ReLU-16            [-1, 256, 8, 8]               0\n",
            "           Conv2d-17            [-1, 256, 8, 8]         590,080\n",
            "             ReLU-18            [-1, 256, 8, 8]               0\n",
            "        MaxPool2d-19            [-1, 256, 4, 4]               0\n",
            "          Dropout-20            [-1, 256, 4, 4]               0\n",
            "           Conv2d-21            [-1, 512, 1, 1]       2,097,664\n",
            "             ReLU-22            [-1, 512, 1, 1]               0\n",
            "           Conv2d-23            [-1, 256, 1, 1]         131,328\n",
            "             ReLU-24            [-1, 256, 1, 1]               0\n",
            "          Dropout-25            [-1, 256, 1, 1]               0\n",
            "           Conv2d-26             [-1, 64, 1, 1]          16,448\n",
            "             ReLU-27             [-1, 64, 1, 1]               0\n",
            "           Conv2d-28             [-1, 10, 1, 1]             650\n",
            "================================================================\n",
            "Total params: 3,373,002\n",
            "Trainable params: 3,373,002\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 3.95\n",
            "Params size (MB): 12.87\n",
            "Estimated Total Size (MB): 16.83\n",
            "----------------------------------------------------------------\n",
            "CNN_model(\n",
            "  (feature_extractor): Sequential(\n",
            "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (10): ReLU(inplace=True)\n",
            "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (12): Dropout2d(p=0.05, inplace=False)\n",
            "    (13): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): ReLU(inplace=True)\n",
            "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.1, inplace=False)\n",
            "    (1): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1))\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.1, inplace=False)\n",
            "    (6): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Conv2d(64, 10, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            ")\n",
            "\n",
            "\n",
            "Finished Training model\n",
            "\n",
            "\n",
            "index is: 199, sum_diff is: 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "index is: 9999, sum_diff is: 9992\n",
            "Accuracy: in test of deepfool 0.0008000000000000229\n",
            "(3, 32, 32)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: in test 0.0989999994635582\n"
          ]
        }
      ],
      "source": [
        "from torch._C import NoneType\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torch import optim\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.utils.data as data\n",
        "from torchsummary import summary\n",
        "\n",
        "# for adversary\n",
        "import copy\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "\n",
        "\n",
        "def load_dataset():\n",
        "    transform_train = transforms.Compose([\n",
        "        # agmentation below\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        # regular normalization\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
        "    ])\n",
        "\n",
        "    # Normalize the test set same as training set without augmentation\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
        "    ])\n",
        "\n",
        "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "    print(f' trainset: {trainset}')\n",
        "    # print(f' trainset shape: {trainset.size()}')\n",
        "\n",
        "    ## script to find mean\n",
        "    # data = trainset.data / 255  # data is numpy array\n",
        "    #\n",
        "    # mean = data.mean(axis=(0, 1, 2))\n",
        "    # std = data.std(axis=(0, 1, 2))\n",
        "    # print(f\"Mean : {mean}   STD: {std}\")  # Mean : [0.491 0.482 0.446]   STD: [0.247 0.243 0.261]\n",
        "\n",
        "    cifar_testset = datasets.CIFAR10(root='./data', train=False, download=True,\n",
        "                                     transform=transform_test)  # transform_test\n",
        "    cifar_trainset, cifar_valset = data.random_split(trainset, [int(len(trainset) * 0.8), int(len(\n",
        "        trainset) * 0.2)])  # split the trainset to trainset and validation set in 80%-20% retio\n",
        "\n",
        "    print('train set len', len(cifar_trainset))\n",
        "    print('validation set len', len(cifar_valset))\n",
        "    print('test set len', len(cifar_testset))\n",
        "\n",
        "    number_workers = 0\n",
        "    if device == torch.device('cuda'):\n",
        "        number_workers = 2\n",
        "    train_loader = data.DataLoader(cifar_trainset, shuffle=True, batch_size=64, num_workers=number_workers)\n",
        "    val_loader = data.DataLoader(cifar_valset, shuffle=False, batch_size=64, num_workers=number_workers)\n",
        "    test_loader = data.DataLoader(cifar_testset, shuffle=False, batch_size=64, num_workers=number_workers)\n",
        "\n",
        "    test_for_adv = data.DataLoader(cifar_testset, shuffle=False, batch_size=1)\n",
        "    train_for_adv = data.DataLoader(cifar_trainset, shuffle=True, batch_size=1)\n",
        "    return train_loader, val_loader, test_loader, test_for_adv, train_for_adv\n",
        "\n",
        "\n",
        "# model3:  with dropout, with batch, without fc layers\n",
        "\n",
        "\n",
        "class CNN_model(nn.Module):  # TODO: fix so I get correct dimensions of output\n",
        "    def __init__(self):\n",
        "        super(CNN_model, self).__init__()\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "\n",
        "            # Conv Layer block 1\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Conv Layer block 2\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout2d(p=0.05),\n",
        "\n",
        "            # Conv Layer block 3\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Conv2d(in_channels=256, out_channels=64, kernel_size=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=64, out_channels=10, kernel_size=1),\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # conv layers\n",
        "        features = self.feature_extractor(x)\n",
        "        # print(\"features shape:\", features.shape)\n",
        "\n",
        "        # final non fully connected\n",
        "        class_scores = self.classifier(features)\n",
        "        # print(\"class_scores shape:\", class_scores.shape)\n",
        "        class_scores = torch.reshape(class_scores, (class_scores.size(dim=0), class_scores.size(dim=1)))\n",
        "        # print(\"class_scores shape:\", class_scores.shape)\n",
        "\n",
        "        return class_scores\n",
        "\n",
        "\n",
        "# Train\n",
        "def train_data(model, epochs, learning_rate, loss_function, train_loader, valid_loader, PATH, filtered_images=[],\n",
        "               patience=4):\n",
        "    loss_arr = []\n",
        "    avg_train_loss_arr, avg_val_loss_arr = [], []\n",
        "    train_acc_arr, val_acc_arr = [], []\n",
        "    # Early stopping  parameters\n",
        "    last_loss = 100  # initializing max loss as high unreachable value\n",
        "    trigger_times = 0\n",
        "    total, correct = 0.0, 0.0\n",
        "\n",
        "    # for i in range(5):\n",
        "    #   data_iter = iter(filtered_images)\n",
        "    #   pert_image_numpy = next(data_iter)[i].cpu().detach().squeeze().numpy()\n",
        "    #   print(pert_image_numpy.shape)\n",
        "    #   plt.figure()\n",
        "    #   plt.imshow(pert_image_numpy.transpose(1, 2, 0))\n",
        "    #   #plt.title(label_pert)\n",
        "    #   plt.show()\n",
        "\n",
        "    # images = next(data_iter)\n",
        "    # print(type(images))\n",
        "    # print(images.shape)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.00012)\n",
        "\n",
        "    # dataiter = iter(train_loader)\n",
        "    # images, labels = dataiter.next()\n",
        "    # print(type(images))\n",
        "    # print(images.shape)\n",
        "    # print(labels.shape)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # defining we're training so can use dropout, batch norm\n",
        "        if len(filtered_images) > 0:\n",
        "            feature_squeezing = True\n",
        "            data_iter = iter(filtered_images)\n",
        "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
        "            if device == torch.device('cuda'):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward and backward propagation\n",
        "            if feature_squeezing:\n",
        "              batch_cur = []\n",
        "              for j in range(len(inputs)):\n",
        "                  cur_img = ndimage.median_filter(inputs[j].cpu().detach(), size=(2, 2, 2), origin=-1)  # mode is reflect by default  # sliding window size is: (2,2), shifted to image right\n",
        "                  batch_cur.append(cur_img)\n",
        "                  #squeezed_images.append(torch.tensor(np.array(batch_cur)))\n",
        "              outputs = model(torch.tensor(np.array(batch_cur)).to(device))\n",
        "              #outputs = model(next(data_iter).to(device))\n",
        "            else:\n",
        "                outputs = model(inputs)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss_arr.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Show progress\n",
        "            if i % 100 == 0 or i == len(train_loader):\n",
        "                # print('[{}/{}, {}/{}] loss: {:.8}'.format(epoch, epochs, i, len(train_loader), loss.item()))\n",
        "                print(\"Iteration: {0} | Loss: {1} | index {2} \".format(epoch, loss.item(), i))\n",
        "\n",
        "            total += inputs.shape[0]\n",
        "            predictions = torch.argmax(outputs.data, dim=1)\n",
        "            correct += torch.sum(predictions == labels).type(torch.float32)\n",
        "\n",
        "        # print(\"total is: {0}, len(train_loader): {1}, correct pred num is: {2}\".format(total, len(train_loader), correct))\n",
        "        train_acc = (correct / total).item()\n",
        "        print('Accuracy: in train', train_acc)\n",
        "        train_acc_arr.append(train_acc)\n",
        "\n",
        "        plot_graph(loss_arr, \"generic network training loss\")\n",
        "        avg_train_loss_arr.append(np.mean(loss_arr))\n",
        "        # Early stopping\n",
        "        current_loss, avg_val_loss, val_acc = validation_data(model, valid_loader)\n",
        "        print('The Current Loss by validation data:', current_loss)\n",
        "        avg_val_loss_arr.append(avg_val_loss.item())\n",
        "        val_acc_arr.append(val_acc)\n",
        "\n",
        "        if current_loss > last_loss:\n",
        "            trigger_times += 1\n",
        "            # print('Trigger Times:', trigger_times)\n",
        "\n",
        "            if trigger_times >= patience:\n",
        "                print('Early stopping!\\nStart to test process.')\n",
        "                break  # exit loop, print data\n",
        "\n",
        "        else:\n",
        "            # print('trigger times did not increase:' , trigger_times)\n",
        "            torch.save(model.state_dict(), PATH)\n",
        "            trigger_times = 0\n",
        "\n",
        "        last_loss = current_loss\n",
        "\n",
        "    model.load_state_dict(torch.load(PATH))\n",
        "    plot_graph(loss_arr, \"generic network training loss\")\n",
        "\n",
        "    title = \"avg train loss vs avg validation loss\"\n",
        "    plot_two_graphs(avg_train_loss_arr, avg_val_loss_arr, title, 'train loss', 'val loss', 'loss',\n",
        "                    'avg_train_loss_vs_avg_validation_loss')\n",
        "\n",
        "    # plt.plot(avg_train_loss_arr, label='train loss')\n",
        "    # # Plot another line on the same chart/graph\n",
        "    # plt.plot(avg_val_loss_arr, label='val loss')\n",
        "    # plt.title(\"avg train loss vs avg validation loss\")\n",
        "    # plt.legend()\n",
        "    # plt.xlabel('epochs')\n",
        "    # plt.ylabel('loss')\n",
        "    # # plt.savefig('./outputs_q1/avg_train_loss_vs_avg_validation_loss.png')\n",
        "    # plt.show()\n",
        "\n",
        "    title = \"avg train acc vs avg validation acc\"\n",
        "    plot_two_graphs(train_acc_arr, val_acc_arr, title, 'train accuracy', 'validation accuracy', 'accuracy',\n",
        "                    'avg_train_acc_vs_avg_validation_acc')\n",
        "    # plt.plot(train_acc_arr, label='train accuracy')\n",
        "    # # Plot another line on the same chart/graph\n",
        "    # plt.plot(val_acc_arr, label='validation accuracy')\n",
        "    # plt.title(\"avg train acc vs avg validation acc\")\n",
        "    # plt.legend()\n",
        "    # plt.xlabel('epochs')\n",
        "    # plt.ylabel('accuracy')\n",
        "    # # plt.savefig('./outputs_q1/avg_train_acc_vs_avg_validation_acc.png')\n",
        "    # plt.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def plot_graph(list, title):\n",
        "    plt.plot(list)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def plot_two_graphs(list1, list2, title, label1, label2, y_label, plot_path):\n",
        "    plt.plot(list1, label=label1)\n",
        "    # Plot another line on the same chart/graph\n",
        "    plt.plot(list2, label=label2)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel(y_label)\n",
        "    plt.savefig('./outputs/'+plot_path+'.png')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def validation_data(model, valid_loader):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    loss_total = 0.0\n",
        "    loss_arr = []\n",
        "\n",
        "    # iterate over test data\n",
        "    with torch.no_grad():  # disable gradients because we only run on test data\n",
        "        for (data, labels) in valid_loader:\n",
        "            # move tensors to GPU if CUDA is available\n",
        "            if device == torch.device('cuda'):\n",
        "                data, labels = data.cuda(), labels.cuda()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(data)\n",
        "            # calculate the batch loss\n",
        "            loss_valid = loss_fn(output, labels)\n",
        "            loss_arr.append(loss_valid.item())\n",
        "            loss_total += loss_valid.item()\n",
        "\n",
        "            total += data.shape[0]\n",
        "            predictions = torch.argmax(output.data, dim=1)\n",
        "            correct += torch.sum(predictions == labels).type(torch.float32)\n",
        "\n",
        "    # plot_graph(loss_arr, \"generic network valid loss\")\n",
        "    acc = (correct / total)\n",
        "    print('Accuracy: in validation', acc.item())\n",
        "\n",
        "    return (loss_total / len(valid_loader)), np.mean(loss_arr), acc.item()\n",
        "\n",
        "\n",
        "def test_data(model, test_loader):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    loss_arr = []\n",
        "\n",
        "    # iterate over test data\n",
        "    with torch.no_grad():  # disable gradients because we only run on test data\n",
        "        for (data, labels) in test_loader:\n",
        "            # move tensors to GPU if CUDA is available\n",
        "            if device == torch.device('cuda'):\n",
        "                data, labels = data.cuda(), labels.cuda()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(data)\n",
        "            # calculate the batch loss\n",
        "            loss_test = loss_fn(output, labels)\n",
        "            loss_arr.append(loss_test.item())\n",
        "\n",
        "            total += data.shape[0]\n",
        "            predictions = torch.argmax(output.data, dim=1)\n",
        "            correct += torch.sum(predictions == labels).type(torch.float32)\n",
        "\n",
        "    plot_graph(loss_arr, \"generic network test loss\")\n",
        "\n",
        "    print('Accuracy: in test', (correct / total).item())\n",
        "\n",
        "\n",
        "def deepfool(image, model, num_classes=10, overshoot=0.02, max_iter=10):  # overshoot prevents vanishing updates\n",
        "    model.eval()\n",
        "    # print('in deepfool before first forward')\n",
        "\n",
        "    image.requires_grad = True\n",
        "    out_orig = model.forward(image.to(device))\n",
        "\n",
        "    temp = np.array(out_orig.cpu().detach().numpy())  # getting image prediction labels\n",
        "    labels = temp.flatten().argsort()[::-1]  # labels indexes from low to high\n",
        "    # print(labels)\n",
        "    # print('after first')\n",
        "\n",
        "    labels = labels[0:num_classes]  # starting with label with highest prediction\n",
        "    label = labels[0]\n",
        "\n",
        "    input_shape = image.detach().shape\n",
        "    pert_image = copy.deepcopy(image)\n",
        "    w = np.zeros(input_shape)\n",
        "    pert_tot = np.zeros(input_shape)\n",
        "    ctr = 0\n",
        "\n",
        "    x = pert_image[None, :].clone().detach().requires_grad_(True).to(device)\n",
        "    x.retain_grad()\n",
        "    out = model.forward(x[0])\n",
        "    # print('after second forward')\n",
        "    label_cur = label\n",
        "\n",
        "    while label_cur == label and ctr < max_iter:  # and x.grad is not None\n",
        "\n",
        "        pert = np.inf\n",
        "        # print('before backward 1')\n",
        "        out[0, labels[0]].backward(\n",
        "            retain_graph=True)  # retain_graph for iterating through the graph after the first time\n",
        "        # print(x.grad)\n",
        "        grad_orig = x.grad.data.cpu().detach().numpy().copy()\n",
        "\n",
        "        for i in range(1, num_classes):  # finding hyperplane which gives smallest difference between all classes\n",
        "            # print('before backward 2')\n",
        "            out[0, labels[i]].backward(retain_graph=True)\n",
        "            cur_grad = x.grad.data.cpu().detach().numpy().copy()\n",
        "\n",
        "            w_cur = cur_grad - grad_orig\n",
        "            out_diff = (out[0, labels[i]] - out[0, labels[\n",
        "                0]]).data.cpu().detach().numpy()  # difference between prediction of original image and perturbed\n",
        "\n",
        "            # using formula to calculate current hyperplane\n",
        "            hyperplane_cur = abs(out_diff) / np.linalg.norm(w_cur.flatten())\n",
        "\n",
        "            # getting minimal change hyperplane\n",
        "            if hyperplane_cur < pert:\n",
        "                pert = hyperplane_cur\n",
        "                w = w_cur\n",
        "\n",
        "        # Added 1e-4 for numerical stability\n",
        "        pert_cur = (pert + 1e-4) * w / np.linalg.norm(w)\n",
        "        # calculating new perturbed image to updated image under min hyperplane such that her projection changed\n",
        "        pert_tot = np.float32(pert_tot + pert_cur)\n",
        "\n",
        "        pert_image = image.cpu().detach() + (1 + overshoot) * torch.from_numpy(pert_tot)\n",
        "\n",
        "        x = pert_image.clone().detach().requires_grad_(True).to(device)\n",
        "        x.retain_grad()\n",
        "        # print('before final forward')\n",
        "        out = model.forward(x[0])\n",
        "        label_cur = np.argmax(out.data.cpu().detach().numpy().flatten())  # label of pert image\n",
        "\n",
        "        ctr += 1\n",
        "\n",
        "    pert_tot = (1 + overshoot) * pert_tot\n",
        "\n",
        "    return pert_tot, ctr, label, label_cur, pert_image\n",
        "\n",
        "\n",
        "def batched_deepfool(model, batch):\n",
        "    sum_diff = 0\n",
        "    adv_images = []\n",
        "    for j in range(len(batch)):  # images one by one\n",
        "        r, loop_i, label_orig, label_pert, pert_image = deepfool(batch[j].unsqueeze(0), model, max_iter=50)\n",
        "        # if label_orig != label_pert:\n",
        "        #     sum_diff += 1\n",
        "        adv_images.append(pert_image.detach().squeeze().to(device))\n",
        "    # print(pert_image.detach().squeeze().to(device).shape)\n",
        "    # print(torch.stack(adv_images).shape)\n",
        "    final = torch.stack(adv_images)\n",
        "    return final\n",
        "\n",
        "\n",
        "def batched_deepfool_train(model, train_loader):  # applying deepfool on each batch and whole train loader\n",
        "    adv_images = []\n",
        "    for i, (batch, labels) in enumerate(train_loader, 0):\n",
        "        batch_cur = []\n",
        "        for j in range(len(batch)):\n",
        "            r, loop_i, label_orig, label_pert, pert_image = deepfool(batch[j].unsqueeze(0), model, max_iter=50)\n",
        "            batch_cur.append(pert_image.detach().squeeze().to(device))\n",
        "        adv_images.append(torch.stack(batch_cur))\n",
        "    print(pert_image.detach().squeeze().to(device).shape)\n",
        "    print(torch.stack(adv_images).shape)\n",
        "    final = torch.stack(adv_images)\n",
        "    torch.save(final, 'final_adv_images.pt')\n",
        "    return final\n",
        "\n",
        "\n",
        "def calling_deepfool(model, xLoader):\n",
        "    sum_diff = 0\n",
        "    misclassified = []\n",
        "    for i, (image, label) in enumerate(xLoader):\n",
        "        if device == torch.device('cuda'):\n",
        "            image, label = image.cuda(), label.cuda()\n",
        "        r, loop_i, label_orig, label_pert, pert_image = deepfool(image, model, max_iter=50)\n",
        "        misclassified.append((pert_image.detach().squeeze(0).to(device), label))\n",
        "        if label_orig != label_pert:\n",
        "            sum_diff += 1\n",
        "        if sum_diff ==200:\n",
        "            print(f'index is: {i}, sum_diff is: {sum_diff}')\n",
        "            #break\n",
        "    print(f'index is: {i}, sum_diff is: {sum_diff}')\n",
        "    acc = 1 - (sum_diff / (i + 1))\n",
        "    print('Accuracy: in test of deepfool', acc)  # accuracy on perturbed images\n",
        "\n",
        "    pert_image_numpy = pert_image.detach().squeeze().numpy()\n",
        "    print(pert_image_numpy.shape)\n",
        "    plt.figure()\n",
        "    plt.imshow((pert_image_numpy * 255).astype(np.uint8).transpose(1, 2, 0))\n",
        "    plt.title(label_pert)\n",
        "    # plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    origin = image.cpu().detach().squeeze().numpy()\n",
        "    plt.imshow((origin * 255).astype(np.uint8).transpose(1, 2, 0))\n",
        "    plt.title(label.item())\n",
        "    # plt.show()\n",
        "\n",
        "    r_new = r.squeeze()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.imshow(r_new.transpose(1, 2, 0))\n",
        "    plt.show()\n",
        "\n",
        "    return misclassified\n",
        "\n",
        "\n",
        "def adversarial_train(model, epochs, learning_rate, loss_function, train_loader, valid_loader, adv_images, PATH):\n",
        "    loss_arr = []\n",
        "    loss_clean_data = []\n",
        "    loss_pert_data = []\n",
        "    acc_clean_data = []\n",
        "    acc_pert_data = []\n",
        "\n",
        "    avg_train_loss_arr = []\n",
        "    avg_loss_clean_data = []\n",
        "    avg_loss_pert_data = []\n",
        "\n",
        "    val_loss = []\n",
        "    val_acc = []\n",
        "    total_train_acc = []\n",
        "    # # Early stopping  parameters\n",
        "    # last_loss = 100  # initializing max loss as high unreachable value\n",
        "    # trigger_times = 0\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.00012)\n",
        "    total, correct = 0.0, 0.0\n",
        "    total_adv, correct_adv = 0.0, 0.0\n",
        "    step = 0\n",
        "    # breakstep = 0\n",
        "    for epoch in range(epochs):\n",
        "        itr = iter(adv_images)\n",
        "        model.train()  # defining we're training so can use dropout, batch norm\n",
        "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
        "            if device == torch.device('cuda'):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Forward and backward propagation\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss_arr.append(loss.item())\n",
        "            loss_clean_data.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            predictions = torch.argmax(outputs.data, dim=1)\n",
        "            correct += torch.sum(predictions == labels).type(torch.float32)\n",
        "            total += train_loader.batch_size\n",
        "\n",
        "            # Adversarial Training\n",
        "\n",
        "            # adv_images = batched_deepfool(model, inputs)\n",
        "            # adv_images[i] = adv_images[i].to(device)\n",
        "            cur_adv_image = next(itr)\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Forward and backward propagation\n",
        "            # pert_image = pert_image.clone().detach().squeeze(1).to(device)\n",
        "            outputs = model(cur_adv_image)\n",
        "            # outputs = model(adv_images)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss_arr.append(loss.item())\n",
        "            loss_pert_data.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            predictions = torch.argmax(outputs.data, dim=1)\n",
        "            correct_adv += torch.sum(predictions == labels).type(torch.float32)\n",
        "            total_adv += train_loader.batch_size\n",
        "\n",
        "            step += 1\n",
        "            if total % 500 == 0:\n",
        "                acc = float(correct) / total\n",
        "                print('[%s] Clean Training accuracy: %.2f%%' % (step, acc * 100))\n",
        "                # total = 0\n",
        "                # correct = 0\n",
        "                accAdv = float(correct_adv) / total_adv\n",
        "                print('[%s] Adv Training accuracy: %.2f%%' % (step, accAdv * 100))\n",
        "\n",
        "                # total_adv = 0\n",
        "                # correct_adv = 0\n",
        "        train_acc = (correct / total).item()\n",
        "        print('Accuracy clean data: in train', train_acc)\n",
        "        acc_clean_data.append(train_acc)\n",
        "\n",
        "        plot_graph(loss_arr, \"generic network training loss\")\n",
        "        avg_train_loss_arr.append(np.mean(loss_arr))\n",
        "\n",
        "        plot_graph(avg_loss_clean_data, \"clean data network training loss\")\n",
        "        avg_loss_clean_data.append(np.mean(loss_clean_data))\n",
        "\n",
        "        plot_graph(avg_loss_pert_data, \"pert data network training loss\")\n",
        "        avg_loss_pert_data.append(np.mean(loss_pert_data))\n",
        "\n",
        "        train_adv_acc = (correct_adv / total_adv).item()\n",
        "        print('Accuracy adv: in train', train_adv_acc)\n",
        "        acc_pert_data.append(train_adv_acc)\n",
        "\n",
        "        current_loss, avg_val_loss, curr_val_acc = validation_data(model, valid_loader)\n",
        "        print('The Current Loss by validation data:', current_loss)\n",
        "        val_loss.append(avg_val_loss)\n",
        "        val_acc.append(curr_val_acc)\n",
        "\n",
        "        total_acc = float(correct + correct_adv) / (total_adv + total)\n",
        "        total_train_acc.append(total_acc)\n",
        "\n",
        "    plot_graph(loss_arr, \"generic network training loss\")\n",
        "    torch.save(model.state_dict(), PATH)\n",
        "\n",
        "    title = \"adversary train loss on clean data vs loss on perturbed data\"\n",
        "    plot_two_graphs(avg_loss_pert_data, avg_loss_clean_data, title, 'loss pert data', 'loss clean data','loss',\n",
        "                    'train_loss_clean_vs_pert')\n",
        "\n",
        "    title = \"accuracy on clean data vs accuracy on perturbed data\"\n",
        "    plot_two_graphs(acc_pert_data, acc_clean_data, title, 'pert data acc', 'clean data acc','accuracy',\n",
        "                    'acc_clean_vs_pert')\n",
        "\n",
        "    title = \"adversary train loss vs avg validation loss\"\n",
        "    plot_two_graphs(avg_train_loss_arr, avg_val_loss, title, 'train loss', 'val loss','loss',\n",
        "                    'adv_train_loss_vs_avg_validation_loss')\n",
        "\n",
        "    title = \"adversary train acc vs avg validation acc\"\n",
        "    plot_two_graphs(total_train_acc, val_acc, title, 'train accuracy', 'validation accuracy', 'accuracy',\n",
        "                    'adv_train_acc_vs_avg_validation_acc')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def feature_squeezing(train_loader):\n",
        "    print(\"in feature squeezing\")\n",
        "    #  median filter implemented in SciPy [38]. In a 2×2 sliding window, the center pixel is always\n",
        "    # located in the lower right. When there are two equal-median values due to the even number of pixels in a window, we\n",
        "    # (arbitrarily) use the greater value as the median\n",
        "    squeezed_images = []\n",
        "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
        "        batch_cur = []\n",
        "        for j in range(len(inputs)):\n",
        "            cur_img = ndimage.median_filter(inputs[j], size=(2, 2, 2), origin=-1)  # mode is reflect by default\n",
        "            # sliding window size is: (2,2), shifted to image right\n",
        "            batch_cur.append(cur_img)\n",
        "\n",
        "        tmp = (torch.Tensor(np.array(batch_cur)), labels)\n",
        "        # tmp = (torch.stack(np.array(batch_cur)), labels)\n",
        "        squeezed_images.append(tmp)\n",
        "        if i == 2:\n",
        "            break\n",
        "        # squeezed_images.append(torch.stack(list((batch_cur, labels)), dim=0))\n",
        "        # squeezed_images.append(torch.stack(torch.Tensor(batch_cur, labels), device=device))\n",
        "    # print((torch.Tensor(cur_img, device=device)).shape)\n",
        "    # print(torch.stack(squeezed_images).shape)\n",
        "    final = torch.stack(squeezed_images)  # turning to one torch, with mulriple dims\n",
        "    torch.save(squeezed_images, 'squeezed_images.pt')\n",
        "    return squeezed_images\n",
        "\n",
        "\n",
        "def feature_squeezing2(train_loader):  # applying deepfool on each batch and whole train loader\n",
        "    squeezed_images = []\n",
        "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
        "        batch_cur = []\n",
        "        for j in range(len(inputs)):\n",
        "            cur_img = ndimage.median_filter(inputs[j], size=(2, 2, 2), origin=-1)  # mode is reflect by default\n",
        "            # sliding window size is: (2,2), shifted to image right\n",
        "            batch_cur.append(cur_img)\n",
        "            # if i<3 and j<3:\n",
        "            #   pert_image_numpy = cur_img.squeeze()\n",
        "            #   print(pert_image_numpy.shape)\n",
        "            #   plt.figure()\n",
        "            #   plt.imshow(pert_image_numpy.transpose(1, 2, 0))\n",
        "            #   #plt.title(label_pert)\n",
        "            #   plt.show()\n",
        "            # \n",
        "            #   image_numpy = inputs[j].detach().squeeze().numpy()\n",
        "            #   print(image_numpy.shape)\n",
        "            #   plt.figure()\n",
        "            #   plt.imshow(image_numpy.transpose(1, 2, 0))\n",
        "            #   #plt.title(label_pert)\n",
        "            #   plt.show()\n",
        "        squeezed_images.append(torch.tensor(np.array(batch_cur)))\n",
        "    print(torch.stack(squeezed_images).shape)\n",
        "    final = torch.stack(squeezed_images)\n",
        "    torch.save(final, 'squeezed_images.pt')\n",
        "    return final\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    global device\n",
        "    device = torch.device('cpu')\n",
        "    # check if cuda is available\n",
        "    train_on_gpu = torch.cuda.is_available()\n",
        "    if train_on_gpu:\n",
        "        device = torch.device('cuda')\n",
        "        print(\"CUDA available. Training on GPU\")\n",
        "    else:\n",
        "        print(\"CUDA is not available. Training on CPU\")\n",
        "\n",
        "    train_loader, val_loader, test_loader, test_for_adv, train_for_adv = load_dataset()\n",
        "    batch_size = 64\n",
        "    max_epochs = 80  # number of steps between evaluations\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    # step 1 - initialize and train model\n",
        "    model = CNN_model().to(device)  # with dropout, batch, without FC layers\n",
        "    summary(model, input_size=(3, 32, 32))\n",
        "    print(model)\n",
        "\n",
        "    PATH = './model.pth'\n",
        "    # model = train_data(model, 100, 0.0001, loss_fn, train_loader, val_loader, PATH)\n",
        "    # torch.save(model.state_dict(), PATH)\n",
        "\n",
        "    model.load_state_dict(torch.load(PATH, map_location=torch.device(device)))\n",
        "    # model.load_state_dict(torch.load(PATH))\n",
        "    # test_data(model, test_loader)\n",
        "    print('\\n\\nFinished Training model\\n\\n')\n",
        "\n",
        "    # step 2: adv attack - deep fool on clean data\n",
        "    # attacked_deepfool = calling_deepfool(model, test_for_adv)\n",
        "    # test_data(model, attacked_deepfool)\n",
        "\n",
        "    # adv_images = batched_deepfool_train(model, train_loader)\n",
        "    # print('\\n\\nFinished creating batched images\\n\\n')\n",
        "\n",
        "    # # step 3: adv training and then retest deepfool\n",
        "    # tensor_adv_images = torch.load('./final_adv_images.pt', map_location=torch.device(device))\n",
        "    # print(tensor_adv_images.shape)\n",
        "\n",
        "    # PATH_ADV = './model_train_adv.pth'\n",
        "    # epochs = 100  # was 100\n",
        "    # # tensor_adv_images = []\n",
        "    # model_adv_train = adversarial_train(model, epochs, 0.0001, loss_fn, train_loader, val_loader, tensor_adv_images,\n",
        "    #                                     PATH_ADV)\n",
        "\n",
        "    # test_data(model_adv_train, test_loader)\n",
        "\n",
        "    # calling_deepfool(model_adv_train, test_for_adv)\n",
        "\n",
        "    # step 4: apply feauture squeezing defense and then test deepfool attack\n",
        "    #feature_squeezing2(train_loader)\n",
        "    #squeezed_images = torch.load('squeezed_images.pt', map_location=torch.device(device))\n",
        "    PATH_SQUEEZED = './model_squeezed.pth'\n",
        "    model_squeezed_train = CNN_model().to(device)\n",
        "    # model_squeezed_train = train_data(model_squeezed_train, 100, 0.0001, loss_fn, train_loader, val_loader,\n",
        "    #                                   PATH_SQUEEZED, squeezed_images)  # squeezed_images instead train loader\n",
        "\n",
        "    model_squeezed_train.load_state_dict(torch.load(PATH_SQUEEZED, map_location=torch.device(device)))\n",
        "\n",
        "    # test_data(model_squeezed_train, test_loader)\n",
        "\n",
        "    # adv attack - deep fool on pre-processed data data\n",
        "    attacked_deepfool_squeezed_im = calling_deepfool(model_squeezed_train, test_for_adv)\n",
        "\n",
        "    test_data(model_squeezed_train, attacked_deepfool_squeezed_im)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DL_project-adversary_attacks.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMp+zyu56RdqX4iu6Ldp4Mt",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}